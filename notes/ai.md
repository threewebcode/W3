# Awesome AI 

## Graph Model

Transformer architecture makes use of attention mechanism applied to natural language processing. Transformer model vectorises the input text through a tokenizer. The relationship and advancement are exlored between language model, neural network, graph and the automatic conversion of text to structured data. 

## Diffusion Models

Diffusion models are widely used for image edit task. The image is decomposed into text embedding or diffusion scrore. The image editing tasks include replace, add and remove operations. Reinforcement learning can directly enhance the reasoning capabilities of large language models without extensive reliances on supervised fine tuning. Memory is a crucial element in the workflow of an intelligent agent. The memory is categorized into sensory memory, short-term memory and long-term memory. The sensor memory corresponds to request inputs or prompts. 

## Model Reasoning

多智能体的协作可以提高大模型的推理能力。目前都是基于训练的推理模型，通过不同的策略来扩展其推理的能力。 大语言模型比较擅长自然语言任务。增加训练数据集和增加模型参数是提高模型能力的两种主要方式。 大语言模型比较擅长文本理解和生成,依赖的是信息的记忆和提取能力，推理需要更多的逻辑步骤。尝试使用机器学习来增强科学研究的假设。现代的机器学习 系统在架构设计、初始化方法和训练机制上存在差异。从观察中发现方程，组合优化是离散方法的一种。从数据中提取向量函数。

## Knowledge Graph of Thought

The knowledge graph of throught is performed by graph executor and tool executor. The graph executor controls what is the next task. The tool executor is selected to execute specific task. The text and visual content can be processed by large language model. The key frames are selected to reduce the total tokens. Various prunning strategies have emgered to reduce computation burdon. Video understanding tasks process a sequence of frames. Each frame is divided into patches. Each patch is encoded into a visual token through vision encoder.  
